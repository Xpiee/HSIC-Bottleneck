{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anubhav\\anaconda3\\envs\\mytorch112\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.0 (default, Nov  6 2019, 16:00:02) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import torchvision\n",
    "import time\n",
    "import copy\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cu116'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc functions (https://github.com/choasma/HSIC-bottleneck/blob/master/source/hsicbt/utils/misc.py)\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return torch.squeeze(torch.eye(num_classes)[y])\n",
    "\n",
    "def get_layer_parameters(model, idx_range):\n",
    "\n",
    "    param_out = []\n",
    "    param_out_name = []\n",
    "    for it, (name, param) in enumerate(model.named_parameters()):\n",
    "        if it in idx_range:\n",
    "            param_out.append(param)\n",
    "            param_out_name.append(name)\n",
    "\n",
    "    return param_out, param_out_name\n",
    "\n",
    "# https://github.com/choasma/HSIC-bottleneck/blob/master/source/hsicbt/utils/meter.py\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Basic meter\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset meter\n",
    "        \"\"\"\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        \"\"\" incremental meter\n",
    "        \"\"\"\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def get_accuracy_hsic(model, dataloader):\n",
    "    \"\"\" Computes the precision@k for the specified values of k\n",
    "        https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    output_list = []\n",
    "    target_list = []\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        output, hiddens = model(data.to(next(model.parameters()).device))\n",
    "        output = output.cpu().detach().numpy()\n",
    "        target = target.cpu().detach().numpy().reshape(-1,1)\n",
    "        output_list.append(output)\n",
    "        target_list.append(target)\n",
    "    output_arr = np.vstack(output_list)\n",
    "    target_arr = np.vstack(target_list)\n",
    "    avg_acc = 0\n",
    "    reorder_list = []\n",
    "    for i in range(10):\n",
    "        indices = np.where(target_arr==i)[0]\n",
    "        select_item = output_arr[indices]\n",
    "        out = np.array([np.argmax(vec) for vec in select_item])\n",
    "        y = np.mean(select_item, axis=0)\n",
    "        while np.argmax(y) in reorder_list:\n",
    "            y[np.argmax(y)] = 0\n",
    "        reorder_list.append(np.argmax(y))\n",
    "        num_correct = np.where(out==np.argmax(y))[0]\n",
    "        accuracy = float(num_correct.shape[0])/float(out.shape[0])\n",
    "        avg_acc += accuracy\n",
    "    avg_acc /= 10.\n",
    "\n",
    "    return avg_acc*100., reorder_list\n",
    "\n",
    "\n",
    "def get_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\" Computes the precision@k for the specified values of k\n",
    "        https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def get_accuracy_epoch(model, dataloader):\n",
    "    \"\"\" Computes the precision@k for the specified values of k\n",
    "        https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    output_list = []\n",
    "    target_list = []\n",
    "    acc = []\n",
    "    loss = []\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "    model = model.to('cuda')\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output, hiddens = model(data)\n",
    "        loss.append(cross_entropy_loss(output, target).cpu().detach().numpy())\n",
    "        acc.append(get_accuracy(output, target)[0].cpu().detach().numpy())\n",
    "    return np.mean(acc), np.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Gausian kernel function to calculate K_X and K_y\n",
    "# gausian kernel, k(x, y) ~ exp(-(1/2)*||x - y||^2/sigma**2 )\n",
    "\n",
    "def distmat(X):\n",
    "    \"\"\" distance matrix\n",
    "        Euclidean Distance Matrix (EDM)\n",
    "        D = abs(a^2 + b^2 - 2ab_T)\n",
    "    \"\"\"\n",
    "    r = torch.sum(X*X, 1)\n",
    "    r = r.view([-1, 1])\n",
    "    a = torch.mm(X, torch.transpose(X,0,1))\n",
    "    D = r.expand_as(a) - 2*a +  torch.transpose(r,0,1).expand_as(a)\n",
    "    D = torch.abs(D)\n",
    "\n",
    "    return D\n",
    "\n",
    "def kernelmat(X, sigma):\n",
    "    \"\"\"\n",
    "    Kernel function\n",
    "\n",
    "    m: training batch size\n",
    "    H: centering matrix:: I_m - (1/m)*1_m.1_m\n",
    "    gausian kernel: k(x, y) ~ exp(-(1/2)*||x - y||^2/sigma**2)\n",
    "    \"\"\"\n",
    "    m = int(X.size()[0]) # batch size\n",
    "    H = torch.eye(m) - (1./m) * torch.ones([m,m])\n",
    "\n",
    "    Dxx = distmat(X)\n",
    "\n",
    "    variance = 2.*sigma*sigma*X.size()[1]            \n",
    "    Kx = torch.exp(-Dxx / variance).type(torch.FloatTensor)   # kernel\n",
    "    Kxc = torch.mm(Kx, H) # kernel function centered with H\n",
    "\n",
    "    return Kxc\n",
    "\n",
    "\n",
    "def hsic_base(x, y, sigma=None, use_cuda=True):\n",
    "    \"\"\"\n",
    "    Implement equation 3 in the paper\n",
    "    HSIC: (m - 1)^-2 . trace(Kx H Ky H)\n",
    "    \"\"\"\n",
    "    m = int(x.size()[0]) # batch size\n",
    "\n",
    "    KxH = kernelmat(x, sigma=sigma)\n",
    "    KyH = kernelmat(y, sigma=sigma)\n",
    "\n",
    "    return torch.trace(KxH @ KyH)/(m - 1)**2\n",
    "\n",
    "# taken from HSIC implementation \n",
    "# https://github.com/choasma/HSIC-bottleneck/blob/9f1fe2447592d61c0ba524aad0ff0820ae2ba9cb/source/hsicbt/core/train_misc.py#L26\n",
    "# def hsic_objective(hidden, h_target, h_data, sigma):\n",
    "\n",
    "#     hsic_hy_val = hsic_base( hidden, h_target, sigma=sigma)\n",
    "#     hsic_hx_val = hsic_base( hidden, h_data,   sigma=sigma)\n",
    "\n",
    "#     return hsic_hx_val, hsic_hy_val\n",
    "\n",
    "def hsic_loss_obj(hidden, h_target, h_data, sigma):\n",
    "    \"\"\"\n",
    "    calculate hsic between input (X) and hidden layer weights\n",
    "    calculate hsic between hidden layer weights and target (Y)\n",
    "\n",
    "    return: hx, hy for calculating loss in training pipeline\n",
    "    \"\"\"\n",
    "    hsic_hx = hsic_base(hidden, h_data, sigma=sigma)\n",
    "    hsic_hy = hsic_base(hidden, h_target, sigma=sigma)\n",
    "\n",
    "    return hsic_hx, hsic_hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# prepare data loader for CIFAR10 and MNIST\n",
    "\n",
    "train_transform = transforms.Compose([transforms.ToTensor()]) # , transforms.Resize(size=(227, 227))\n",
    "valid_transform = train_transform\n",
    "\n",
    "train_set = CIFAR10('./data/cifar10', train=True,\n",
    "                  download=True, transform=train_transform)\n",
    "valid_set = CIFAR10('./data/cifar10', train=False,\n",
    "                  download=True, transform=valid_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create primitive conv block with conv2d, bn, and activation.\n",
    "\n",
    "def get_activation(atype):\n",
    "\n",
    "    if atype=='relu':\n",
    "        nonlinear = nn.ReLU()\n",
    "    elif atype=='tanh':\n",
    "        nonlinear = nn.Tanh() \n",
    "    elif atype=='sigmoid':\n",
    "        nonlinear = nn.Sigmoid() \n",
    "    elif atype=='elu':\n",
    "        nonlinear = nn.ELU()\n",
    "\n",
    "    return nonlinear\n",
    "\n",
    "def makeblock_conv(in_chs, out_chs, atype, stride=1):\n",
    "\n",
    "    layer = nn.Conv2d(in_channels=in_chs, \n",
    "        out_channels=out_chs, kernel_size=5, stride=stride)\n",
    "    bn = nn.BatchNorm2d(out_chs, affine=False)\n",
    "    nonlinear = get_activation(atype)\n",
    "\n",
    "    return nn.Sequential(*[layer, bn, nonlinear])\n",
    "\n",
    "def makeblock_dense(in_dim, out_dim, atype):\n",
    "    \n",
    "    layer = nn.Linear(in_dim, out_dim)\n",
    "    bn = nn.BatchNorm1d(out_dim, affine=False)\n",
    "    nonlinear = get_activation(atype)\n",
    "    out = nn.Sequential(*[layer, bn, nonlinear])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_width=784, hidden_width=64, n_layers=5, atype='relu', \n",
    "        last_hidden_width=None, data_code='cifar10', **kwargs):\n",
    "        super(ModelConv, self).__init__()\n",
    "    \n",
    "        block_list = []\n",
    "        is_conv = False\n",
    "\n",
    "        if data_code == 'cifar10':\n",
    "            in_ch = 3\n",
    "            last_hidden_width = 10\n",
    "        elif data_code == 'mnist':\n",
    "            in_ch = 1\n",
    "\n",
    "        last_hw = hidden_width\n",
    "        if last_hidden_width:\n",
    "            last_hw = last_hidden_width\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            block = makeblock_conv(hidden_width, hidden_width, atype)\n",
    "            block_list.append(block)\n",
    "\n",
    "        self.input_layer    = makeblock_conv(in_ch, hidden_width, atype)\n",
    "        self.sequence_layer = nn.Sequential(*block_list)\n",
    "        if data_code == 'mnist':\n",
    "            dim = 512\n",
    "        elif data_code == 'cifar10':\n",
    "            dim = 8192\n",
    "\n",
    "        self.output_layer = makeblock_dense(dim, last_hw, atype)\n",
    "\n",
    "        self.is_conv = is_conv\n",
    "        self.in_width = in_width\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output_list = []\n",
    "        \n",
    "        x = self.input_layer(x)\n",
    "        output_list.append(x)\n",
    "        \n",
    "        for block in self.sequence_layer:\n",
    "            x = block(x.clone())\n",
    "            output_list.append(x)\n",
    "            \n",
    "        x = x.view(-1, np.prod(x.size()[1:]))\n",
    "\n",
    "        # print(x.size())\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        output_list.append(x)\n",
    "\n",
    "        return x, output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda'\n",
    "# model = ModelConv()\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic_train(cepoch, model, data_loader, config_dict):\n",
    "\n",
    "    # cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "    prec1 = total_loss = hx_l = hy_l = -1\n",
    "\n",
    "    batch_acc    = AverageMeter()\n",
    "    batch_loss   = AverageMeter()\n",
    "    batch_hischx = AverageMeter()\n",
    "    batch_hischy = AverageMeter()\n",
    "\n",
    "    batch_log = {}\n",
    "    batch_log['batch_acc'] = []\n",
    "    batch_log['batch_loss'] = []\n",
    "    batch_log['batch_hsic_hx'] = []\n",
    "    batch_log['batch_hsic_hy'] = []\n",
    "\n",
    "    model = model.to(config_dict['device'])\n",
    "\n",
    "    n_data = config_dict['batch_size'] * len(data_loader)\n",
    "\n",
    "    # sigma_optimizer = optim.SGD([sigma_tensor], lr=1E-5)\n",
    "\n",
    "    # for batch_idx, (data, target) in enumerate(data_loader):\n",
    "    pbar = tqdm(enumerate(data_loader), total=n_data/config_dict['batch_size'], ncols=120)\n",
    "    for batch_idx, (data, target) in pbar:\n",
    "\n",
    "        # if os.environ.get('HSICBT_DEBUG')=='4':\n",
    "        #     if batch_idx > 5:\n",
    "        #         break\n",
    "                \n",
    "        data   = data.to(config_dict['device'])\n",
    "        target = target.to(config_dict['device'])\n",
    "        output, hiddens = model(data)\n",
    "\n",
    "        h_target = target.view(-1,1)\n",
    "        h_target = to_categorical(h_target, num_classes=10).float()\n",
    "        h_data = data.view(-1, np.prod(data.size()[1:]))\n",
    "\n",
    "        idx_range = []\n",
    "        it = 0\n",
    "\n",
    "        # So the batchnorm is not learnable, making only @,b at layer\n",
    "        for i in range(len(hiddens)):\n",
    "            idx_range.append(np.arange(it, it+2).tolist())\n",
    "            it += 2\n",
    "    \n",
    "        for i in range(len(hiddens)):\n",
    "            \n",
    "            output, hiddens = model(data)\n",
    "            params, param_names = get_layer_parameters(model=model, idx_range=idx_range[i]) # so we only optimize one layer at a time\n",
    "            optimizer = optim.SGD(params, lr = config_dict['learning_rate'], momentum=.9, weight_decay=0.001)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if len(hiddens[i].size()) > 2:\n",
    "                hiddens[i] = hiddens[i].view(-1, np.prod(hiddens[i].size()[1:]))\n",
    "\n",
    "            hx_l, hy_l = hsic_loss_obj(\n",
    "                    hiddens[i],\n",
    "                    h_target=h_target.float(),\n",
    "                    h_data=h_data,\n",
    "                    sigma=config_dict['sigma'],\n",
    "            )\n",
    "            #print(torch.max(hiddens[i]).cpu().detach().numpy(), torch.min(hiddens[i]).cpu().detach().numpy(), torch.std(hiddens[i]).cpu().detach().numpy())\n",
    "            loss = (hx_l - config_dict['lambda_y']*hy_l)\n",
    "            if i == 0:\n",
    "                loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            optimizer.step()\n",
    "            # sigma_optimizer.step()\n",
    "        # if config_dict['hsic_solve']:\n",
    "        # prec1, reorder_list = get_accuracy_hsic(model, data_loader)\n",
    "        batch_acc.update(prec1)\n",
    "        batch_loss.update(total_loss)\n",
    "        batch_hischx.update(hx_l.cpu().detach().numpy())\n",
    "        batch_hischy.update(hy_l.cpu().detach().numpy())\n",
    "\n",
    "        # print('H_hx:{H_hx:.8f} H_hy:{H_hy:.8f}'.format(H_hx = hx_l, H_hy = hy_l))\n",
    "\n",
    "        # # # preparation log information and print progress # # #\n",
    "\n",
    "        msg = 'Train Epoch: {cepoch} [ {cidx:5d}/{tolidx:5d} ({perc:2d}%)] H_hx:{H_hx:.8f} H_hy:{H_hy:.8f}'.format(\n",
    "                        cepoch = cepoch,  \n",
    "                        cidx = (batch_idx+1)*config_dict['batch_size'], \n",
    "                        tolidx = n_data,\n",
    "                        perc = int(100. * (batch_idx+1)*config_dict['batch_size']/n_data), \n",
    "                        H_hx = batch_hischx.avg, \n",
    "                        H_hy = batch_hischy.avg,\n",
    "                )\n",
    "\n",
    "        if ((batch_idx+1) % config_dict['log_batch_interval'] == 0):\n",
    "\n",
    "            batch_log['batch_acc'].append(batch_acc.avg)\n",
    "            batch_log['batch_loss'].append(batch_loss.avg)\n",
    "            batch_log['batch_hsic_hx'].append(batch_hischx.avg)\n",
    "            batch_log['batch_hsic_hy'].append(batch_hischy.avg)\n",
    "\n",
    "        pbar.set_description(msg)\n",
    "\n",
    "    return batch_log, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = transforms.Compose([transforms.ToTensor()]) # , transforms.Resize(size=(227, 227))\n",
    "# valid_transform = train_transform\n",
    "\n",
    "# train_set = MNIST('./data/mnist', train=True,\n",
    "#                   download=True, transform=train_transform)\n",
    "# valid_set = MNIST('./data/mnist', train=False,\n",
    "#                   download=True, transform=valid_transform)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "# val_loader = torch.utils.data.DataLoader(valid_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(dataFolderPath='./data/mnist', train=True, download=True, batchSize=64):\n",
    "    \n",
    "    train_transform = transforms.Compose([transforms.ToTensor()]) # , transforms.Resize(size=(227, 227))\n",
    "    valid_transform = train_transform\n",
    "\n",
    "    train_set = MNIST(dataFolderPath, train=train,\n",
    "                  download=download, transform=train_transform)\n",
    "    valid_set = MNIST(dataFolderPath, train=False,\n",
    "                  download=True, transform=valid_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batchSize, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(valid_set, batch_size=batchSize, shuffle=False)    \n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def load_cifar10(dataFolderPath='./data/cifar10', train=True, download=True, batchSize=64):\n",
    "    \n",
    "    train_transform = transforms.Compose([transforms.ToTensor()]) # , transforms.Resize(size=(227, 227))\n",
    "    valid_transform = train_transform\n",
    "\n",
    "    train_set = MNIST(dataFolderPath, train=train,\n",
    "                  download=download, transform=train_transform)\n",
    "    valid_set = MNIST(dataFolderPath, train=False,\n",
    "                  download=True, transform=valid_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batchSize, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(valid_set, batch_size=batchSize, shuffle=False)    \n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def get_data(data_name, batch_size):\n",
    "\n",
    "    if data_name=='cifar10':\n",
    "        dataPath = './data/cifar10'\n",
    "        train_loader, test_loader=load_cifar10(dataPath, batchSize=batch_size)\n",
    "\n",
    "    elif data_name=='mnist':\n",
    "        dataPath = './data/mnist'\n",
    "        train_loader, test_loader=load_mnist(dataPath, batchSize=batch_size)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [ 60032/60032 (100%)] H_hx:0.00001800 H_hy:0.00002529: 100%|█████████| 469/469.0 [00:32<00:00, 14.33it/s]\n",
      "Train Epoch: 1 [ 60032/60032 (100%)] H_hx:0.00001836 H_hy:0.00002600: 100%|█████████| 469/469.0 [00:31<00:00, 15.13it/s]\n",
      "Train Epoch: 2 [ 60032/60032 (100%)] H_hx:0.00001865 H_hy:0.00002647: 100%|█████████| 469/469.0 [00:30<00:00, 15.19it/s]\n",
      "Train Epoch: 3 [ 60032/60032 (100%)] H_hx:0.00001898 H_hy:0.00002702: 100%|█████████| 469/469.0 [00:30<00:00, 15.21it/s]\n",
      "Train Epoch: 4 [ 60032/60032 (100%)] H_hx:0.00001940 H_hy:0.00002773: 100%|█████████| 469/469.0 [00:33<00:00, 14.09it/s]\n"
     ]
    }
   ],
   "source": [
    "config_dict = {}\n",
    "config_dict['batch_size'] = 128\n",
    "config_dict['learning_rate'] = 0.001\n",
    "config_dict['lambda_y'] = 50.\n",
    "config_dict['sigma'] = 2.\n",
    "config_dict['task'] = 'hsic-train'\n",
    "config_dict['device'] = 'cuda'\n",
    "config_dict['log_batch_interval'] = 10\n",
    "\n",
    "# # # data prepreation\n",
    "# train_loader, test_loader = get_dataset_from_code('mnist', 128)\n",
    "\n",
    "# # # simple fully-connected model\n",
    "model = ModelConv(hidden_width=32,\n",
    "                    n_layers=5,\n",
    "                    atype='relu',\n",
    "                    last_hidden_width=None,\n",
    "                    data_code='mnist')\n",
    "\n",
    "# # # start to train\n",
    "epochs = 5\n",
    "for cepoch in range(epochs):\n",
    "    # you can also re-write hsic_train function\n",
    "    batch_log, model = hsic_train(cepoch, model, train_loader, config_dict)\n",
    "    # print(get_accuracy_epoch(model, train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strftime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mn:\\Work\\Huawei\\HSIC_basic\\hsic_basic_implment.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/Work/Huawei/HSIC_basic/hsic_basic_implment.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcolor\u001b[39;00m \u001b[39mimport\u001b[39;00m print_emph, print_highlight\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/Work/Huawei/HSIC_basic/hsic_basic_implment.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m misc\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/Work/Huawei/HSIC_basic/hsic_basic_implment.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m path\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/Work/Huawei/HSIC_basic/hsic_basic_implment.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatetime\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/n%3A/Work/Huawei/HSIC_basic/hsic_basic_implment.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# TTYPE_STANDARD = 'backprop'\u001b[39;00m\n",
      "File \u001b[1;32mn:\\Work\\Huawei\\HSIC_basic\\utils\\path.py:3\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpath\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconst\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcolor\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mglob\u001b[39;00m\n",
      "File \u001b[1;32mn:\\Work\\Huawei\\HSIC_basic\\utils\\const.py:20\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m DEBUG_MODE \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     19\u001b[0m TIMESTAMP \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mHSICBT_TIMESTAMP\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 20\u001b[0m current_time_stamp \u001b[39m=\u001b[39m get_current_timestamp()\n\u001b[0;32m     21\u001b[0m \u001b[39m#if not os.environ.get(TIMESTAMP) or \\\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m#   not os.environ.get(TIMESTAMP) == current_time_stamp:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m#    os.environ[TIMESTAMP] = current_time_stamp\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(TIMESTAMP):\n",
      "File \u001b[1;32mn:\\Work\\Huawei\\HSIC_basic\\utils\\misc.py:4\u001b[0m, in \u001b[0;36mget_current_timestamp\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_current_timestamp\u001b[39m():\n\u001b[1;32m----> 4\u001b[0m     \u001b[39mreturn\u001b[39;00m strftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39my\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%\u001b[39m\u001b[39mH\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'strftime' is not defined"
     ]
    }
   ],
   "source": [
    "from color import print_emph, print_highlight\n",
    "from utils import misc\n",
    "from utils import path\n",
    "import datetime\n",
    "# TTYPE_STANDARD = 'backprop'\n",
    "TTYPE_HSICTRAIN = 'hsictrain'\n",
    "# TTYPE_FORMAT = 'format'\n",
    "# TTYPE_UNFORMAT = 'unformat'\n",
    "\n",
    "# 1. train the HSIC model with last hidden demensions != 10 and save the model.T_destination\n",
    "## 2. Create ensemble model with hsic model + linear model with softmax and train for 10 epochs\n",
    "## without backprop (only update last layer params by SGD optim.)\n",
    "\n",
    "def training_hsic(config_dict):\n",
    "\n",
    "    print_emph(\"HSIC-Bottleneck training\")\n",
    "    code_name = [config_dict['task'], TTYPE_HSICTRAIN, config_dict['data_code'], config_dict['exp_index']]\n",
    "\n",
    "    train_loader, test_loader = get_data(\n",
    "        config_dict['data_code'], config_dict['batch_size'])\n",
    "    #torch.manual_seed(config_dict['seed'])\n",
    "    # model = model_distribution(config_dict)\n",
    "\n",
    "    model = ModelConv(**config_dict)\n",
    "    nepoch = config_dict['epochs']\n",
    "    epoch_range = range(1, nepoch+1)\n",
    "\n",
    "    batch_log_list = []\n",
    "    epoch_log_dict = {}\n",
    "    epoch_log_dict['train_acc'] = []\n",
    "    epoch_log_dict['test_acc'] = []\n",
    "\n",
    "    for cepoch in epoch_range:\n",
    "\n",
    "        log = hsic_train(cepoch, model, train_loader, config_dict)\n",
    "\n",
    "        batch_log_list.append(log)\n",
    "\n",
    "        # save with each indexed\n",
    "        filename = os.path.join(config_dict['data_code'], datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "        filename = \"{}---{:04d}.pt\".format(filename, cepoch)\n",
    "        save_model(model, )\n",
    "\n",
    "        log_dict = {}\n",
    "        log_dict['batch_log_list'] = batch_log_list\n",
    "        log_dict['epoch_log_dict'] = epoch_log_dict\n",
    "        log_dict['config_dict'] = config_dict\n",
    "        save_logs(log_dict, get_log_filepath(*code_name))\n",
    "\n",
    "    return batch_log_list, epoch_log_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_accuracy_epoch(model, train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mn:\\Work\\Huawei\\HSIC_basic\\hsic_basic_implment.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/n%3A/Work/Huawei/HSIC_basic/hsic_basic_implment.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m prec1, reorder_list \u001b[39m=\u001b[39m get_accuracy_hsic(model, val_loader)\n",
      "\u001b[1;32mn:\\Work\\Huawei\\HSIC_basic\\hsic_basic_implment.ipynb Cell 14\u001b[0m in \u001b[0;36mget_accuracy_hsic\u001b[1;34m(model, dataloader)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/Work/Huawei/HSIC_basic/hsic_basic_implment.ipynb#X30sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(select_item, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/Work/Huawei/HSIC_basic/hsic_basic_implment.ipynb#X30sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mwhile\u001b[39;00m np\u001b[39m.\u001b[39margmax(y) \u001b[39min\u001b[39;00m reorder_list:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/n%3A/Work/Huawei/HSIC_basic/hsic_basic_implment.ipynb#X30sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     y[np\u001b[39m.\u001b[39;49margmax(y)] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/Work/Huawei/HSIC_basic/hsic_basic_implment.ipynb#X30sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m reorder_list\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39margmax(y))\n\u001b[0;32m     <a href='vscode-notebook-cell:/n%3A/Work/Huawei/HSIC_basic/hsic_basic_implment.ipynb#X30sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m num_correct \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(out\u001b[39m==\u001b[39mnp\u001b[39m.\u001b[39margmax(y))[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\mytorch112\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1216\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m \u001b[39mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[39m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m kwds \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mkeepdims\u001b[39m\u001b[39m'\u001b[39m: keepdims} \u001b[39mif\u001b[39;00m keepdims \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39m_NoValue \u001b[39melse\u001b[39;00m {}\n\u001b[1;32m-> 1216\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32mc:\\Users\\Anubhav\\anaconda3\\envs\\mytorch112\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prec1, reorder_list = get_accuracy_hsic(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\" Computes the precision@k for the specified values of k\n",
    "        https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def get_accuracy_epoch(model, dataloader):\n",
    "    \"\"\" Computes the precision@k for the specified values of k\n",
    "        https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    output_list = []\n",
    "    target_list = []\n",
    "    acc = []\n",
    "    loss = []\n",
    "    cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "    model = model.to('cuda')\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output, hiddens = model(data)\n",
    "        loss.append(cross_entropy_loss(output, target).cpu().detach().numpy())\n",
    "        acc.append(get_accuracy(output, target)[0].cpu().detach().numpy())\n",
    "    return np.mean(acc), np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10.237372, 2.476208)\n"
     ]
    }
   ],
   "source": [
    "print(get_accuracy_epoch(model, train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/391.0 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 6400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config_dict = {}\n",
    "config_dict['batch_size'] = 32\n",
    "config_dict['learning_rate'] = 0.001\n",
    "config_dict['lambda_y'] = 100\n",
    "config_dict['sigma'] = 2\n",
    "config_dict['task'] = 'hsic-train'\n",
    "config_dict['device'] = 'cuda'\n",
    "config_dict['log_batch_interval'] = 10\n",
    "\n",
    "# # # data prepreation\n",
    "# train_loader, test_loader = get_dataset_from_code('mnist', 128)\n",
    "\n",
    "# # # simple fully-connected model\n",
    "model = ModelConv(hidden_width=16,\n",
    "                    n_layers=2,\n",
    "                    atype='relu',\n",
    "                    last_hidden_width=10,\n",
    "                    data_code='cifar10')\n",
    "\n",
    "# # # start to train\n",
    "epochs = 0\n",
    "# for cepoch in range(epochs):\n",
    "#     # you can also re-write hsic_train function\n",
    "#     hsic_train(cepoch, model, train_loader, config_dict)\n",
    "\n",
    "data_loader = train_loader\n",
    "\n",
    "# def hsic_train(cepoch, model, data_loader, config_dict):\n",
    "\n",
    "# cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "prec1 = total_loss = hx_l = hy_l = -1\n",
    "\n",
    "batch_acc    = AverageMeter()\n",
    "batch_loss   = AverageMeter()\n",
    "batch_hischx = AverageMeter()\n",
    "batch_hischy = AverageMeter()\n",
    "\n",
    "batch_log = {}\n",
    "batch_log['batch_acc'] = []\n",
    "batch_log['batch_loss'] = []\n",
    "batch_log['batch_hsic_hx'] = []\n",
    "batch_log['batch_hsic_hy'] = []\n",
    "\n",
    "model = model.to(config_dict['device'])\n",
    "\n",
    "n_data = config_dict['batch_size'] * len(data_loader)\n",
    "\n",
    "# sigma_optimizer = optim.SGD([sigma_tensor], lr=1E-5)\n",
    "\n",
    "# for batch_idx, (data, target) in enumerate(data_loader):\n",
    "pbar = tqdm(enumerate(data_loader), total=n_data/config_dict['batch_size'], ncols=120)\n",
    "for batch_idx, (data, target) in pbar:\n",
    "\n",
    "    # if os.environ.get('HSICBT_DEBUG')=='4':\n",
    "    #     if batch_idx > 5:\n",
    "    #         break\n",
    "            \n",
    "    data   = data.to(config_dict['device'])\n",
    "    target = target.to(config_dict['device'])\n",
    "    output, hiddens = model(data)\n",
    "\n",
    "    h_target = target.view(-1,1)\n",
    "    h_target = to_categorical(h_target, num_classes=10).float()\n",
    "    h_data = data.view(-1, np.prod(data.size()[1:]))\n",
    "\n",
    "    idx_range = []\n",
    "    it = 0\n",
    "\n",
    "    # So the batchnorm is not learnable, making only @,b at layer\n",
    "    for i in range(len(hiddens)):\n",
    "        idx_range.append(np.arange(it, it+2).tolist())\n",
    "        it += 2\n",
    "\n",
    "\n",
    "    break\n",
    "\n",
    "    # for i in range(len(hiddens)):\n",
    "        \n",
    "    #     # output, hiddens = model(data)\n",
    "    #     params, param_names = get_layer_parameters(model=model, idx_range=idx_range[i]) # so we only optimize one layer at a time\n",
    "    #     optimizer = optim.SGD(params, lr = config_dict['learning_rate'], momentum=.9, weight_decay=0.001)\n",
    "    #     optimizer.zero_grad()\n",
    "        \n",
    "    #     if len(hiddens[i].size()) > 2:\n",
    "    #         hiddens[i] = hiddens[i].view(-1, np.prod(hiddens[i].size()[1:]))\n",
    "\n",
    "    #     hx_l, hy_l = hsic_loss_obj(\n",
    "    #             hiddens[i],\n",
    "    #             h_target=h_target.float(),\n",
    "    #             h_data=h_data,\n",
    "    #             sigma=config_dict['sigma'],\n",
    "    #     )\n",
    "    #     #print(torch.max(hiddens[i]).cpu().detach().numpy(), torch.min(hiddens[i]).cpu().detach().numpy(), torch.std(hiddens[i]).cpu().detach().numpy())\n",
    "    #     loss = hx_l - config_dict['lambda_y']*hy_l\n",
    "    #     if i == 0:\n",
    "    #         loss.backward(retain_graph=True)\n",
    "    #     else:\n",
    "    #         loss.backward()\n",
    "    #     optimizer.step()\n",
    "    #     # sigma_optimizer.step()\n",
    "    # # if config_dict['hsic_solve']:\n",
    "    # #     prec1, reorder_list = misc.get_accuracy_hsic(model, data_loader)\n",
    "    # batch_acc.update(prec1)\n",
    "    # batch_loss.update(total_loss)\n",
    "    # batch_hischx.update(hx_l.cpu().detach().numpy())\n",
    "    # batch_hischy.update(hy_l.cpu().detach().numpy())\n",
    "\n",
    "    # # # # preparation log information and print progress # # #\n",
    "\n",
    "    # msg = 'Train Epoch: {cepoch} [ {cidx:5d}/{tolidx:5d} ({perc:2d}%)] H_hx:{H_hx:.4f} H_hy:{H_hy:.4f}'.format(\n",
    "    #                 cepoch = cepoch,  \n",
    "    #                 cidx = (batch_idx+1)*config_dict['batch_size'], \n",
    "    #                 tolidx = n_data,\n",
    "    #                 perc = int(100. * (batch_idx+1)*config_dict['batch_size']/n_data), \n",
    "    #                 H_hx = batch_hischx.avg, \n",
    "    #                 H_hy = batch_hischy.avg,\n",
    "    #         )\n",
    "\n",
    "    # if ((batch_idx+1) % config_dict['log_batch_interval'] == 0):\n",
    "\n",
    "    #     batch_log['batch_acc'].append(batch_loss.avg)\n",
    "    #     batch_log['batch_loss'].append(batch_acc.avg)\n",
    "    #     batch_log['batch_hsic_hx'].append(batch_hischx.avg)\n",
    "    #     batch_log['batch_hsic_hy'].append(batch_hischy.avg)\n",
    "\n",
    "    # pbar.set_description(msg)\n",
    "\n",
    "    # return batch_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, param_names = get_layer_parameters(model=model, idx_range=idx_range[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 10])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(hiddens)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create neural network model\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 5,\n",
    "                                stride = 2, padding = 2)\n",
    "        self.layer2 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 5, \n",
    "                               stride = 2, padding = 2)\n",
    "        self.layer3 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 5, \n",
    "                               stride = 2, padding = 2)\n",
    "        self.mxpool = nn.MaxPool2d(2, 2)\n",
    "        self.mxpool1 = nn.MaxPool2d(2, 2, padding = 1)\n",
    "\n",
    "        self.linear1 = nn.Linear(4096, 1024)\n",
    "        self.linear2 = nn.Linear(1024, 1024)\n",
    "        self.linear3 = nn.Linear(1024, 100)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.mxpool(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.mxpool1(x)\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.mxpool(x)\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x) # using nn.CrossEntropy() that combines nn.LogSoftmax() and nn.NLLLoss() in one single class\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('mytorch112')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "840df3acb9dd349666656d740704624be9680c29e8fba9a7b1e52a4e4eaf3e37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
