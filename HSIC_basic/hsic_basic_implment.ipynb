{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.1 (default, Dec 11 2020, 09:29:25) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import torchvision\n",
    "import time\n",
    "import copy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1+cu110'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc functions (https://github.com/choasma/HSIC-bottleneck/blob/master/source/hsicbt/utils/misc.py)\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
    "    return torch.squeeze(torch.eye(num_classes)[y])\n",
    "\n",
    "def get_layer_parameters(model, idx_range):\n",
    "\n",
    "    param_out = []\n",
    "    param_out_name = []\n",
    "    for it, (name, param) in enumerate(model.named_parameters()):\n",
    "        if it in idx_range:\n",
    "            param_out.append(param)\n",
    "            param_out_name.append(name)\n",
    "\n",
    "    return param_out, param_out_name\n",
    "\n",
    "# https://github.com/choasma/HSIC-bottleneck/blob/master/source/hsicbt/utils/meter.py\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Basic meter\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" reset meter\n",
    "        \"\"\"\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        \"\"\" incremental meter\n",
    "        \"\"\"\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Gausian kernel function to calculate K_X and K_y\n",
    "# gausian kernel, k(x, y) ~ exp(-(1/2)*||x - y||^2/sigma**2 )\n",
    "\n",
    "def distmat(X):\n",
    "    \"\"\" distance matrix\n",
    "        Euclidean Distance Matrix (EDM)\n",
    "        D = abs(a^2 + b^2 - 2ab_T)\n",
    "    \"\"\"\n",
    "    r = torch.sum(X*X, 1)\n",
    "    r = r.view([-1, 1])\n",
    "    a = torch.mm(X, torch.transpose(X,0,1))\n",
    "    D = r.expand_as(a) - 2*a +  torch.transpose(r,0,1).expand_as(a)\n",
    "    D = torch.abs(D)\n",
    "\n",
    "    return D\n",
    "\n",
    "def kernelmat(X, sigma):\n",
    "    \"\"\"\n",
    "    Kernel function\n",
    "\n",
    "    m: training batch size\n",
    "    H: centering matrix:: I_m - (1/m)*1_m.1_m\n",
    "    gausian kernel: k(x, y) ~ exp(-(1/2)*||x - y||^2/sigma**2)\n",
    "    \"\"\"\n",
    "    m = int(X.size()[0]) # batch size\n",
    "    H = torch.eye(m) - (1./m) * torch.ones([m,m])\n",
    "\n",
    "    Dxx = distmat(X)\n",
    "\n",
    "    variance = 2.*sigma*sigma*X.size()[1]            \n",
    "    Kx = torch.exp( -Dxx / variance).type(torch.FloatTensor)   # kernel\n",
    "    Kxc = torch.mm(Kx, H) # kernel function centered with H\n",
    "\n",
    "    return Kxc\n",
    "\n",
    "\n",
    "def hsic_base(x, y, sigma=None, use_cuda=True):\n",
    "    \"\"\"\n",
    "    Implement equation 3 in the paper\n",
    "    HSIC: (m - 1)^-2 . trace(Kx H Ky H)\n",
    "    \"\"\"\n",
    "    m = int(x.size()[0]) # batch size\n",
    "\n",
    "    KxH = kernelmat(x, sigma=sigma)\n",
    "    KyH = kernelmat(y, sigma=sigma)\n",
    "\n",
    "    return torch.trace(KxH @ KyH)/(m - 1)**2\n",
    "\n",
    "# taken from HSIC implementation \n",
    "# https://github.com/choasma/HSIC-bottleneck/blob/9f1fe2447592d61c0ba524aad0ff0820ae2ba9cb/source/hsicbt/core/train_misc.py#L26\n",
    "# def hsic_objective(hidden, h_target, h_data, sigma):\n",
    "\n",
    "#     hsic_hy_val = hsic_base( hidden, h_target, sigma=sigma)\n",
    "#     hsic_hx_val = hsic_base( hidden, h_data,   sigma=sigma)\n",
    "\n",
    "#     return hsic_hx_val, hsic_hy_val\n",
    "\n",
    "def hsic_loss_obj(hidden, h_target, h_data, sigma):\n",
    "    \"\"\"\n",
    "    calculate hsic between input (X) and hidden layer weights\n",
    "    calculate hsic between hidden layer weights and target (Y)\n",
    "\n",
    "    return: hx, hy for calculating loss in training pipeline\n",
    "    \"\"\"\n",
    "    hsic_hx = hsic_base(hidden, h_data, sigma=sigma)\n",
    "    hsic_hy = hsic_base(hidden, h_target, sigma=sigma)\n",
    "\n",
    "    return hsic_hx, hsic_hy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar100\\cifar-100-python.tar.gz\n",
      "Failed download. Trying https -> http instead. Downloading http://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar100\\cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar100\\cifar-100-python.tar.gz to ./data/cifar100\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# prepare data loader for CIFAR10 and MNIST\n",
    "\n",
    "train_transform = transforms.Compose([transforms.ToTensor(), transforms.Resize(size=(227, 227))])\n",
    "valid_transform = train_transform\n",
    "\n",
    "train_set = CIFAR100('./data/cifar100', train=True,\n",
    "                  download=True, transform=train_transform)\n",
    "valid_set = CIFAR100('./data/cifar100', train=False,\n",
    "                  download=True, transform=valid_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(valid_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create primitive conv block with conv2d, bn, and activation.\n",
    "\n",
    "def makeblock_conv(in_chs, out_chs, atype, stride=1):\n",
    "\n",
    "    layer = nn.Conv2d(in_channels=in_chs, \n",
    "        out_channels=out_chs, kernel_size=5, stride=stride)\n",
    "    bn = nn.BatchNorm2d(out_chs, affine=False)\n",
    "    nonlinear = nn.ReLU()\n",
    "\n",
    "    return nn.Sequential(*[layer, bn, nonlinear])\n",
    "\n",
    "def makeblock_dense(in_dim, out_dim, atype):\n",
    "    \n",
    "    layer = nn.Linear(in_dim, out_dim)\n",
    "    bn = nn.BatchNorm1d(out_dim, affine=False)\n",
    "    nonlinear = nn.ReLU()\n",
    "    out = nn.Sequential(*[layer, bn, nonlinear])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_width=784, hidden_width=64, n_layers=5, atype='relu', \n",
    "        last_hidden_width=None, data_code='cifar10', **kwargs):\n",
    "        super(ModelConv, self).__init__()\n",
    "    \n",
    "        block_list = []\n",
    "        is_conv = False\n",
    "\n",
    "        if data_code == 'cifar10':\n",
    "            in_ch = 3\n",
    "            last_hidden_width = 10\n",
    "        elif data_code == 'mnist':\n",
    "            in_ch == 1\n",
    "\n",
    "        last_hw = hidden_width\n",
    "        if last_hidden_width:\n",
    "            last_hw = last_hidden_width\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            block = makeblock_conv(hidden_width, hidden_width, atype)\n",
    "            block_list.append(block)\n",
    "\n",
    "        self.input_layer    = makeblock_conv(in_ch, hidden_width, atype)\n",
    "        self.sequence_layer = nn.Sequential(*block_list)\n",
    "        if data_code == 'mnist':\n",
    "            dim = 128\n",
    "        elif data_code == 'cifar10':\n",
    "            dim = 960\n",
    "\n",
    "        self.output_layer   = makeblock_dense(dim, last_hw, atype)\n",
    "\n",
    "        self.is_conv = is_conv\n",
    "        self.in_width = in_width\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output_list = []\n",
    "        \n",
    "        x = self.input_layer(x)\n",
    "        output_list.append(x)\n",
    "        \n",
    "        for block in self.sequence_layer:\n",
    "            x = block(x)\n",
    "            output_list.append(x)\n",
    "            \n",
    "        x = x.view(-1, np.prod(x.size()[1:]))\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        output_list.append(x)\n",
    "\n",
    "        return x, output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConv(\n",
       "  (input_layer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (sequence_layer): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Sequential(\n",
       "    (0): Linear(in_features=960, out_features=10, bias=True)\n",
       "    (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model = ModelConv()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic_train(cepoch, model, data_loader, config_dict):\n",
    "\n",
    "    # cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "    prec1 = total_loss = hx_l = hy_l = -1\n",
    "\n",
    "    batch_acc    = AverageMeter()\n",
    "    batch_loss   = AverageMeter()\n",
    "    batch_hischx = AverageMeter()\n",
    "    batch_hischy = AverageMeter()\n",
    "\n",
    "    batch_log = {}\n",
    "    batch_log['batch_acc'] = []\n",
    "    batch_log['batch_loss'] = []\n",
    "    batch_log['batch_hsic_hx'] = []\n",
    "    batch_log['batch_hsic_hy'] = []\n",
    "\n",
    "    model = model.to(config_dict['device'])\n",
    "\n",
    "    n_data = config_dict['batch_size'] * len(data_loader)\n",
    "\n",
    "    # sigma_optimizer = optim.SGD([sigma_tensor], lr=1E-5)\n",
    "\n",
    "    # for batch_idx, (data, target) in enumerate(data_loader):\n",
    "    pbar = tqdm(enumerate(data_loader), total=n_data/config_dict['batch_size'], ncols=120)\n",
    "    for batch_idx, (data, target) in pbar:\n",
    "\n",
    "        if os.environ.get('HSICBT_DEBUG')=='4':\n",
    "            if batch_idx > 5:\n",
    "                break\n",
    "                \n",
    "        data   = data.to(config_dict['device'])\n",
    "        target = target.to(config_dict['device'])\n",
    "        output, hiddens = model(data)\n",
    "\n",
    "        h_target = target.view(-1,1)\n",
    "        h_target = to_categorical(h_target, num_classes=10).float()\n",
    "        h_data = data.view(-1, np.prod(data.size()[1:]))\n",
    "\n",
    "        idx_range = []\n",
    "        it = 0\n",
    "\n",
    "        # So the batchnorm is not learnable, making only @,b at layer\n",
    "        for i in range(len(hiddens)):\n",
    "            idx_range.append(np.arange(it, it+2).tolist())\n",
    "            it += 2\n",
    "    \n",
    "        for i in range(len(hiddens)):\n",
    "            \n",
    "            output, hiddens = model(data)\n",
    "            params, param_names = get_layer_parameters(model=model, idx_range=idx_range[i]) # so we only optimize one layer at a time\n",
    "            optimizer = optim.SGD(params, lr = config_dict['learning_rate'], momentum=.9, weight_decay=0.001)\n",
    "            optimizer.zero_grad()\n",
    "            if len(hiddens[i].size()) > 2:\n",
    "                hiddens[i] = hiddens[i].view(-1, np.prod(hiddens[i].size()[1:]))\n",
    "\n",
    "            hx_l, hy_l = hsic_loss_obj(\n",
    "                    hiddens[i],\n",
    "                    h_target=h_target.float(),\n",
    "                    h_data=h_data,\n",
    "                    sigma=config_dict['sigma'],\n",
    "            )\n",
    "            #print(torch.max(hiddens[i]).cpu().detach().numpy(), torch.min(hiddens[i]).cpu().detach().numpy(), torch.std(hiddens[i]).cpu().detach().numpy())\n",
    "            loss = hx_l - config_dict['lambda_y']*hy_l\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # sigma_optimizer.step()\n",
    "        # if config_dict['hsic_solve']:\n",
    "        #     prec1, reorder_list = misc.get_accuracy_hsic(model, data_loader)\n",
    "        batch_acc.update(prec1)\n",
    "        batch_loss.update(total_loss)\n",
    "        batch_hischx.update(hx_l.cpu().detach().numpy())\n",
    "        batch_hischy.update(hy_l.cpu().detach().numpy())\n",
    "\n",
    "        # # # preparation log information and print progress # # #\n",
    "\n",
    "        msg = 'Train Epoch: {cepoch} [ {cidx:5d}/{tolidx:5d} ({perc:2d}%)] H_hx:{H_hx:.4f} H_hy:{H_hy:.4f}'.format(\n",
    "                        cepoch = cepoch,  \n",
    "                        cidx = (batch_idx+1)*config_dict['batch_size'], \n",
    "                        tolidx = n_data,\n",
    "                        perc = int(100. * (batch_idx+1)*config_dict['batch_size']/n_data), \n",
    "                        H_hx = batch_hischx.avg, \n",
    "                        H_hy = batch_hischy.avg,\n",
    "                )\n",
    "\n",
    "        if ((batch_idx+1) % config_dict['log_batch_interval'] == 0):\n",
    "\n",
    "            batch_log['batch_acc'].append(batch_loss.avg)\n",
    "            batch_log['batch_loss'].append(batch_acc.avg)\n",
    "            batch_log['batch_hsic_hx'].append(batch_hischx.avg)\n",
    "            batch_log['batch_hsic_hy'].append(batch_hischy.avg)\n",
    "\n",
    "        pbar.set_description(msg)\n",
    "\n",
    "        # if cepoch==1:\n",
    "        #     data = activations_extraction(model, data_loader)\n",
    "        #     _code_name = [config_dict['task'], TTYPE_HSICTRAIN, config_dict['data_code']+\"_batch\", batch_idx]\n",
    "        #     filepath = get_act_path(*_code_name)\n",
    "        #     save_logs(data, filepath)\n",
    "\n",
    "    return batch_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create neural network model\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 5,\n",
    "                                stride = 2, padding = 2)\n",
    "        self.layer2 = nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size = 5, \n",
    "                               stride = 2, padding = 2)\n",
    "        self.layer3 = nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size = 5, \n",
    "                               stride = 2, padding = 2)\n",
    "        self.mxpool = nn.MaxPool2d(2, 2)\n",
    "        self.mxpool1 = nn.MaxPool2d(2, 2, padding = 1)\n",
    "\n",
    "        self.linear1 = nn.Linear(4096, 1024)\n",
    "        self.linear2 = nn.Linear(1024, 1024)\n",
    "        self.linear3 = nn.Linear(1024, 100)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.mxpool(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.mxpool1(x)\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.mxpool(x)\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x) # using nn.CrossEntropy() that combines nn.LogSoftmax() and nn.NLLLoss() in one single class\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('mypytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c6afc48870520fb5f8712d7fd98b5ae62e5cb7b5ae0ef5091718f87d94e61c1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
