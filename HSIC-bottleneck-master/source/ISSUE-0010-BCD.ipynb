{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anubhav\\anaconda3\\envs\\mytorch112\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"source\")\n",
    "import torch\n",
    "import numpy as np\n",
    "from hsicbt.utils.misc import get_layer_parameters\n",
    "from hsicbt.model.mhlinear import ModelLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelLinear(\n",
      "  (input_layer): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      "  (sequence_layer): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
      "    (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# # # our model\n",
    "model = ModelLinear(last_hidden_width=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Preparation\n",
    "batch_size = 32\n",
    "train_x = torch.randn(batch_size, 784)\n",
    "train_y = torch.randint(0,10,(batch_size,)).long()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "output, hiddens = model(train_x)\n",
    "idx_range = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Proposed approach ============\n"
     ]
    }
   ],
   "source": [
    "print(\"========== Proposed approach ============\")\n",
    "layer_idx = 3 # let's say third layer\n",
    "it = 0 # It's ugly, the aim is trying to query the parameters of the model at each layer, which is skip 2 because weight and bias\n",
    "for i in range(len(hiddens)):\n",
    "    idx_range.append(np.arange(it, it+2).tolist())\n",
    "    it += 2\n",
    "params, param_names = get_layer_parameters(model=model, idx_range=idx_range[layer_idx])\n",
    "optimizer = torch.optim.SGD(params, lr=0.1, momentum=.9, weight_decay=0.001) # we only expose the weights at layer_idx to optimizer\n",
    "loss = criterion(output, train_y)\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sequence_layer.2.0.weight', 'sequence_layer.2.0.bias']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer.0.weight\n",
      "input_layer.0.bias\n",
      "sequence_layer.0.0.weight\n",
      "sequence_layer.0.0.bias\n",
      "sequence_layer.1.0.weight\n",
      "sequence_layer.1.0.bias\n",
      "sequence_layer.2.0.weight\n",
      "sequence_layer.2.0.bias\n",
      "sequence_layer.3.0.weight\n",
      "sequence_layer.3.0.bias\n",
      "sequence_layer.4.0.weight\n",
      "sequence_layer.4.0.bias\n",
      "output_layer.0.weight\n",
      "output_layer.0.bias\n"
     ]
    }
   ],
   "source": [
    "for it, (para, paraname) in enumerate(model.named_parameters()):\n",
    "\n",
    "    print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0330, -0.0772, -0.0422, -0.0251, -0.0954, -0.0680,  0.0271, -0.1060,\n",
       "        -0.1188,  0.1011], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff of the model weight and bias (Only layer:3 are updated)\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.00833749771118164, 6.29425048828125e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# # # Check before&after weight update\n",
    "norm_before_step = []\n",
    "for p in model.parameters():\n",
    "    norm_before_step.append(torch.norm(p).item())\n",
    "optimizer.step() # let's apply weights on model\n",
    "norm_after_step = []\n",
    "for p in model.parameters():\n",
    "    norm_after_step.append(torch.norm(p).item())\n",
    "# # # Difference checking\n",
    "print(f\"Diff of the model weight and bias (Only layer:{layer_idx} are updated)\")\n",
    "print([val[0]-val[1] for val in zip(norm_before_step, norm_after_step) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Standard backprop ============\n"
     ]
    }
   ],
   "source": [
    "print(\"========== Standard backprop ============\")\n",
    "model = ModelLinear()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=.9, weight_decay=0.001)\n",
    "output, hiddens = model(train_x)\n",
    "loss = criterion(output, train_y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_before_step = []\n",
    "for p in model.parameters():\n",
    "    norm_before_step.append(torch.norm(p).item())\n",
    "optimizer.step()\n",
    "norm_after_step = []\n",
    "for p in model.parameters():\n",
    "    norm_after_step.append(torch.norm(p).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff of the model weight and bias in backprop (All weights should be changed)\n",
      "[-3.0573997497558594, 3.217160701751709e-05, -0.18912172317504883, 0.00010091066360473633, -0.12189292907714844, 0.00011330842971801758, -0.07697868347167969, 0.00011420249938964844, -0.04471778869628906, 0.00011616945266723633, -0.02904987335205078, 0.00010466575622558594, -0.018402099609375, 0.00011450052261352539]\n"
     ]
    }
   ],
   "source": [
    "print(\"Diff of the model weight and bias in backprop (All weights should be changed)\")\n",
    "print([val[0]-val[1] for val in zip(norm_before_step, norm_after_step) ])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('mytorch112')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "840df3acb9dd349666656d740704624be9680c29e8fba9a7b1e52a4e4eaf3e37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
